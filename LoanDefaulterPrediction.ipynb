{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv('/home/abhay/Downloads/MyProjects/loan-default-prediction/train_v2.csv', header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/abhay/Downloads/MyProjects/loan-default-prediction/train_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droped all the object type columns because they were conntaining some very large data values\n",
    "# which must be outlier or unintended data columns for our modelling.\n",
    "\n",
    "obj_cols = df.columns[df.dtypes == 'object']\n",
    "df = df.drop(list(obj_cols),axis=1)\n",
    "\n",
    "# Drop columns which contains sinngle value so, they don't have any value\n",
    "for i in df.columns:\n",
    "    if len(set(df[i]))==1:\n",
    "        df.drop(labels=[i], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['loss']!=0,'loss']=1\n",
    "y = df['loss']\n",
    "ids = df['id']\n",
    "predictors = df.drop(['loss','id'],axis=1)\n",
    "cols = list(predictors.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = predictors.corr()\n",
    "iters = range(len(corr_matrix.columns) - 1)\n",
    "drop_cols = []\n",
    "\n",
    "# Iterate through the correlation matrix and compare correlations\n",
    "for i in iters:\n",
    "    for j in range(i):\n",
    "        item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "        col = item.columns\n",
    "        row = item.index\n",
    "        val = abs(item.values)\n",
    "\n",
    "        # If correlation exceeds the threshold\n",
    "        if val >= 0.6:\n",
    "            # Print the correlated features and the correlation value\n",
    "            #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "            drop_cols.append(col.values[0])\n",
    "\n",
    "# Drop one of each pair of correlated columns\n",
    "drops = set(drop_cols)\n",
    "predictors = predictors.drop(columns = drops)\n",
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(predictors.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handling Missing Data using various techniques \n",
    "\n",
    "#1. Dropping the missing values\n",
    "#df = df.dropna(axis=0)\n",
    "\n",
    "#2. Imputing the meand/median values\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# #imp = Imputer(missing_values=0, strategy='mean')\n",
    "# cleaned_predictors = pd.DataFrame(imputer.fit_transform(predictors),columns = cols)\n",
    "\n",
    "#3.Imputing using knn \n",
    "\n",
    "# from sklearn.impute import KNNImputer\n",
    "# # start the KNN training\n",
    "# imputer = KNNImputer(missing_values=np.nan,n_neighbors = 5)\n",
    "# f = imputer.fit(x_train)\n",
    "# g = f.transform(x_test)\n",
    "\n",
    "#4. \n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "imputer = IterativeImputer(random_state=0,missing_values=np.nan, n_nearest_features=5)\n",
    "cleaned_predictors = pd.DataFrame(imputer.fit_transform(predictors),columns = features)\n",
    "\n",
    "#x.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_predictors.to_csv('/home/abhay/features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(cleaned_predictors,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(copy=False)\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = x_train.shape[1])\n",
    "#transformed_predictors = pca.fit_transform(cleaned_predictors)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = pca.explained_variance_ratio_\n",
    "s = 0\n",
    "for i in arr:\n",
    "    s+=i\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\n",
    "         \"Nearest Neighbors\",\n",
    "         \"Decision Tree\",\n",
    "         \"Random Forest\", \n",
    "         \"Neural Net\", \n",
    "         \"AdaBoost\",\n",
    "         \"Naive Bayes\", \n",
    "         \"QDA\"\n",
    "        ]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(max_depth=5,min_samples_leaf=100),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "   ]\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(x_train, y_train)\n",
    "    score = clf.score(x_test, y_test)\n",
    "    print('Classification score for '+ name +' Algo is :- '+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Decision Tree Classifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# params = {'criterion': ['gini', 'entropy'],'max_depth': np.arange(5,10), 'min_samples_leaf':np.arange(100,500,50),\n",
    "#           'max_features' :['auto', 'sqrt', 'log2']}\n",
    "# dtc = DecisionTreeClassifier()\n",
    "# grid = GridSearchCV(estimator = dtc,cv = 7, param_grid=params, refit=True)\n",
    "# grid.fit(x_train,y_train)\n",
    "# print(grid.best_score_)\n",
    "# print(grid.best_estimator_)\n",
    "\n",
    "# Accuracy :- 0.9072446459035366\n",
    "#Note :- Can't chooose this model as this is overfitting the data so not performing well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "params = { \n",
    "    'n_estimators': [200, 700],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator = rfc,cv = 5, param_grid=params, refit=True)\n",
    "grid.fit(x_train,y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = '/home/abhay/finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/home/abhay/Downloads/MyProjects/loan-default-prediction/test_v2.csv', usecols = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "missing_imputer = IterativeImputer(random_state=0,missing_values=np.nan, n_nearest_features=5)\n",
    "\n",
    "#from sklearn.impute import SimpleImputer\n",
    "# missing_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "test_predictors = pd.DataFrame(missing_imputer.fit_transform(df1),columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(copy=False)\n",
    "#sc.fit_transform(test_predictors)\n",
    "test_predictors = scaler.fit_transform(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = len(features)) \n",
    "transformed_test = pca.fit_transform(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = pca.explained_variance_ratio_\n",
    "s = 0\n",
    "for i in ar:\n",
    "    s+=i\n",
    "print(s)\n",
    "\n",
    "#test_predictors.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty = loaded_model.predict(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
